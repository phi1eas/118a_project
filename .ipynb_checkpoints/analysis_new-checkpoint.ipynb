{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Questions_\n",
    "- Large and small datasets, what is better: same relative or absolute train size?\n",
    "  (or choose subset of data a priori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Imports\n",
    "\n",
    "import copy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import string\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import neighbors\n",
    "from sklearn import neural_network\n",
    "from sklearn import model_selection\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Config\n",
    "\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "MAX_DATA_SIZE = 500\n",
    "\n",
    "RND_SEED = 1\n",
    "\n",
    "CLF_DICT       = {'logreg': linear_model.LogisticRegression(),\n",
    "                  'knn':    neighbors.KNeighborsClassifier(),\n",
    "                  'rf':     ensemble.RandomForestClassifier(),\n",
    "                  'svm':    svm.SVC()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Methods and classes\n",
    "\n",
    "def size_info():\n",
    "    ### Size info\n",
    "    print(\"Data sizes:\")\n",
    "    for data_name, data_tuple in all_data_dict.items():\n",
    "        print(\"\\n{}:\\nX: {}\\ny: {}\".format(data_name, data_tuple[0].shape, data_tuple[1].shape))\n",
    "\n",
    "def shuffle(df):\n",
    "    \"\"\"\n",
    "    Shuffles dataset using seed specified in RND_SEED (see config part above).\n",
    "    \n",
    "        df:  Dataset to be shuffled.\n",
    "        \n",
    "    Returns shuffled dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    return(df.sample(frac=1, random_state=np.random.RandomState(seed=RND_SEED)))\n",
    "\n",
    "def init_clf(clf_name, clf_dict=CLF_DICT):\n",
    "    return(copy.deepcopy(clf_dict[clf_name]))\n",
    "\n",
    "class MagicSearcher:\n",
    "    \n",
    "    \"\"\"\n",
    "    Finds hyperparams for set of datasets and set of classifiers with specified hyperparam grids\n",
    "    \"\"\"\n",
    "    \n",
    "    data_dict       = None\n",
    "    clf_param_dict  = None\n",
    "    cv              = None\n",
    "    n_jobs          = None\n",
    "    verbose         = None\n",
    "    method          = None\n",
    "    \n",
    "    # Randomized Search\n",
    "    n_iter = None\n",
    "    \n",
    "    # Results\n",
    "    searcher_obj_dict = None\n",
    "    best_params_dict  = None\n",
    "    scores_df         = None # Used for plotting\n",
    "    \n",
    "    def __init__(self, clf_param_dict, data_dict=None, cv=5, n_jobs=4, verbose=False, method='grid_search'):\n",
    "        self.data_dict      = data_dict\n",
    "        self.clf_param_dict = clf_param_dict\n",
    "        self.cv             = cv\n",
    "        self.n_jobs         = n_jobs\n",
    "        self.verbose        = verbose\n",
    "        self.method         = method\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    def save(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    def get_plot_by_clf(self, clf_name, x_param_name, figsize=(14, 8)):\n",
    "        clf_scores_df = self.scores_df[clf_name]\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        sns_plot = sns.pointplot(x_param_name, 'scores_mean', data=clf_scores_df, hue='data_name', markers='.')\n",
    "        return(sns_plot)\n",
    "    \n",
    "    def create_scores_df(self):\n",
    "        # Prepare score df dict\n",
    "        clf_scores_df_dict = dict()\n",
    "        for clf_name, param_dict in self.clf_param_dict.items():\n",
    "            columns = ['data_name']\n",
    "            columns.extend(list(self.clf_param_dict[clf_name].keys()))\n",
    "            columns.extend(['scores_mean', 'scores_sd'])\n",
    "\n",
    "            clf_scores_df_dict[clf_name] = pd.DataFrame(columns=columns)\n",
    "\n",
    "        for data_name, clf_searcher_obj_dict in self.searcher_obj_dict.items():\n",
    "            for clf_name, searcher_obj in clf_searcher_obj_dict.items():\n",
    "                grid_scores = searcher_obj.skl_search_obj.grid_scores_\n",
    "\n",
    "                for grid_score in grid_scores:\n",
    "                    param_comb_dict = grid_score[0]\n",
    "                    scores_mean = np.mean(grid_score[2])\n",
    "                    scores_sd = np.std(grid_score[2])\n",
    "\n",
    "                    row = {'data_name': data_name,\n",
    "                           'scores_mean': scores_mean,\n",
    "                           'scores_sd': scores_sd}\n",
    "\n",
    "                    for param_name, param_val in param_comb_dict.items():\n",
    "                        row[param_name] = param_val\n",
    "\n",
    "                    clf_scores_df_dict[clf_name] = clf_scores_df_dict[clf_name].append(row, ignore_index = True)\n",
    "        self.scores_df = clf_scores_df_dict\n",
    "        \n",
    "    def search(self, data_dict=None, n_iter=None):\n",
    "        if self.method == 'randomized_search' and n_iter is None:\n",
    "            raise Exception('You need to specify n_iter for randomized search')\n",
    "        self.n_iter = n_iter\n",
    "            \n",
    "        if (self.data_dict is None) and (data_dict is None):\n",
    "            raise Exception('You need to specify data!') \n",
    "        \n",
    "        searcher_obj_dict = dict()\n",
    "        best_params_dict = dict()\n",
    "        for data_name, data_tuple in self.data_dict.items():\n",
    "            print(\"Working on dataset {} ...\".format(data_name))\n",
    "            \n",
    "            X = data_tuple[0]\n",
    "            y = data_tuple[1]\n",
    "            \n",
    "            searcher_obj_dict[data_name] = dict()\n",
    "            best_params_dict[data_name] = dict()\n",
    "            for clf_name, param_dict in clf_param_dict.items():\n",
    "                print(\"  Doing {} magic ...\".format(clf_name))\n",
    "                searcher_obj = ParamSearcher(X, y, clf_name, param_dict, self.method, self.n_jobs, self.cv, self.verbose)\n",
    "                searcher_obj.search()\n",
    "                searcher_obj_dict[data_name][clf_name] = searcher_obj\n",
    "                \n",
    "                best_params_dict[data_name][clf_name] = searcher_obj.best_params_\n",
    "        self.searcher_obj_dict = searcher_obj_dict\n",
    "        self.best_params_dict = best_params_dict\n",
    "        \n",
    "        self.create_scores_df()\n",
    "\n",
    "class ParamSearcher:\n",
    "    \n",
    "    \"\"\"\n",
    "    Finds hyperparams for one dataset and one classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    X = None\n",
    "    y = None\n",
    "    clf_name = None\n",
    "    param_dict = None\n",
    "    method = None\n",
    "    verbose = None\n",
    "    n_jobs = None\n",
    "    cv = None\n",
    "    \n",
    "    # Randomized Search\n",
    "    n_iter = None\n",
    "    \n",
    "    # Results\n",
    "    skl_search_obj = None\n",
    "    best_params_ = None\n",
    "    \n",
    "    def __init__(self, X, y, clf_name, param_dict, method='grid_search', n_jobs=4, cv=5, verbose=False):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.clf_name = clf_name\n",
    "        self.param_dict = param_dict\n",
    "        self.method = method\n",
    "        self.n_jobs = n_jobs\n",
    "        self.cv = cv\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def check_params(self):\n",
    "        # Check for exceeded hard limits of some params\n",
    "        if self.clf_name == 'knn':\n",
    "            if 'n_neighbors' in self.param_dict:\n",
    "                max_n_neighbors = int(np.floor(self.X.shape[0]/self.cv)-1)\n",
    "                if np.any(self.param_dict['n_neighbors'] > max_n_neighbors):\n",
    "                    print('ParamSearcher: knn: some n_neighbors > n_samples/cv-1. Restricting range to n_samples/cv-1.')\n",
    "                    self.param_dict['n_neighbors'][self.param_dict['n_neighbors'] > max_n_neighbors] = max_n_neighbors\n",
    "                    \n",
    "                    # Remove duplicates\n",
    "                    self.param_dict['n_neighbors'] = np.unique(self.param_dict['n_neighbors'])\n",
    "        elif self.clf_name == 'rf':\n",
    "            if 'max_features' in self.param_dict:\n",
    "                if np.any(self.param_dict['max_features'] > self.X.shape[1]):\n",
    "                    print('ParamSearcher: rf: some max_features > n_features. Restricting range to max_features.')\n",
    "                    self.param_dict['max_features'][self.param_dict['max_features'] > self.X.shape[1]] = self.X.shape[1]\n",
    "                    \n",
    "                    # Remove duplicates\n",
    "                    self.param_dict['max_features'] = np.unique(self.param_dict['max_features'])\n",
    "                    \n",
    "                    \n",
    "    def search(self, n_iter=None):\n",
    "        self.check_params()\n",
    "        \n",
    "        if self.method == 'grid_search':\n",
    "            skl_search_obj = GridSearchCV(estimator  = init_clf(self.clf_name),\n",
    "                                          param_grid = self.param_dict,\n",
    "                                          n_jobs     = self.n_jobs,\n",
    "                                          cv         = self.cv,\n",
    "                                          verbose    = self.verbose)\n",
    "        elif self.method == 'randomized_search':\n",
    "            if n_iter is None:\n",
    "                raise Exception('You need to specify n_iter for randomized search')\n",
    "            self.n_iter = n_iter\n",
    "            \n",
    "            skl_search_obj = RandomizedSearchCV(estimator           = init_clf(self.clf_name),\n",
    "                                                 param_distributions = self.param_dict,\n",
    "                                                 n_iter              = n_iter,\n",
    "                                                 n_jobs              = self.n_jobs,\n",
    "                                                 cv                  = self.cv,\n",
    "                                                 verbose             = self.verbose)\n",
    "        skl_search_obj.fit(self.X, self.y)\n",
    "        self.skl_search_obj = skl_search_obj\n",
    "        self.best_params_ = skl_search_obj.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sizes:\n",
      "\n",
      "wdbc:\n",
      "X: (569, 30)\n",
      "y: (569,)\n",
      "\n",
      "income:\n",
      "X: (32561, 108)\n",
      "y: (32561,)\n",
      "\n",
      "iris:\n",
      "X: (150, 4)\n",
      "y: (150,)\n",
      "\n",
      "covtype:\n",
      "X: (581011, 54)\n",
      "y: (581011,)\n",
      "\n",
      "letter:\n",
      "X: (20000, 16)\n",
      "y: (20000,)\n"
     ]
    }
   ],
   "source": [
    "### Load data\n",
    "## iris\n",
    "iris_X = pd.DataFrame(datasets.load_iris()['data'])\n",
    "iris_y = pd.Series(datasets.load_iris()['target'])\n",
    "\n",
    "\n",
    "## wdbc\n",
    "wdbc_X_and_y = pd.read_csv('data/wdbc.data', header = None).iloc[:, 1:] # drop ID, then first col = y\n",
    "wdbc_y = wdbc_X_and_y.iloc[:, 0]\n",
    "wdbc_X = wdbc_X_and_y.iloc[:, 1:]\n",
    "\n",
    "wdbc_y = wdbc_y.map({'B': -1, 'M': 1}) # Transform y from (B, M) to (-1, 1)\n",
    "\n",
    "\n",
    "## income\n",
    "# Load, prepare, and shuffle adult income data\n",
    "income_X_and_y = pd.read_csv('data/adult.data', header=None)\n",
    "income_X_and_y.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "                         'marital-status', 'occupation', 'relationship',\n",
    "                         'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "                         'native-country', 'income']\n",
    "\n",
    "# one-hot encode categorical variables\n",
    "income_categorical_vars = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                           'relationship', 'race', 'sex', 'native-country']\n",
    "income_X_and_y_onehot = pd.DataFrame()\n",
    "for var in income_categorical_vars:\n",
    "    dummy_coded_var_df = pd.get_dummies(income_X_and_y[var], prefix=var)\n",
    "    income_X_and_y_onehot = pd.concat([income_X_and_y_onehot, dummy_coded_var_df], axis=1)\n",
    "\n",
    "# add remaining columns to one-hot encoded df\n",
    "income_X_and_y = pd.concat([income_X_and_y_onehot,\n",
    "                            income_X_and_y.loc[:, income_X_and_y.columns[\n",
    "                                np.logical_not(np.in1d(income_X_and_y.columns, income_categorical_vars))]]],\n",
    "                           axis=1)\n",
    "\n",
    "income_y = income_X_and_y.loc[:, 'income']\n",
    "income_X = income_X_and_y.drop('income', axis=1)\n",
    "\n",
    "# Transform y from (<=50K, >50K) to (-1, 1)\n",
    "income_y = income_y.map({' <=50K': -1, ' >50K': 1})\n",
    "\n",
    "\n",
    "## Letter\n",
    "letter_X_and_y = pd.read_csv('data/letter.data', header=None)\n",
    "letter_X = letter_X_and_y.iloc[:, 1:]\n",
    "letter_y = letter_X_and_y.iloc[:, 0]\n",
    "\n",
    "# Transform y from A:M -> -1 and N:Z -> 1\n",
    "def alph_to_cat(letter):\n",
    "    if str.upper(letter) in list(string.ascii_uppercase[:13]):\n",
    "        return(1)\n",
    "    elif str.upper(letter) in list(string.ascii_uppercase[13:]):\n",
    "        return(-1)\n",
    "    \n",
    "letter_y = letter_y.map(alph_to_cat)\n",
    "\n",
    "## covtype\n",
    "covtype_X_and_y = pd.read_csv('data/covtype.data')\n",
    "covtype_X = covtype_X_and_y.iloc[:, :-1]\n",
    "covtype_y = covtype_X_and_y.iloc[:, -1]\n",
    "\n",
    "covtype_y = covtype_y.map({7:1}).fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "all_data_dict = {'wdbc':      (wdbc_X, wdbc_y),\n",
    "                 'income':    (income_X, income_y),\n",
    "                 'iris':      (iris_X, iris_y),\n",
    "                 'covtype':   (covtype_X, covtype_y),\n",
    "                 'letter':    (letter_X, letter_y)}\n",
    "\n",
    "### Shuffle\n",
    "for data_name, data_tuple in all_data_dict.items():\n",
    "    X = data_tuple[0]\n",
    "    y = data_tuple[1]\n",
    "    \n",
    "    X = shuffle(X)\n",
    "    y = shuffle(y)\n",
    "    \n",
    "    all_data_dict[data_name] = (X, y)\n",
    "\n",
    "size_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sizes:\n",
      "\n",
      "wdbc:\n",
      "X: (569, 30)\n",
      "y: (569,)\n",
      "\n",
      "income:\n",
      "X: (1000, 108)\n",
      "y: (1000,)\n",
      "\n",
      "iris:\n",
      "X: (150, 4)\n",
      "y: (150,)\n",
      "\n",
      "covtype:\n",
      "X: (1000, 54)\n",
      "y: (1000,)\n",
      "\n",
      "letter:\n",
      "X: (1000, 16)\n",
      "y: (1000,)\n"
     ]
    }
   ],
   "source": [
    "### Limit dataset sizes\n",
    "for data_name, data_tuple in all_data_dict.items():\n",
    "    X = data_tuple[0]\n",
    "    y = data_tuple[1]\n",
    "    \n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    \n",
    "    if y.shape[0] > MAX_DATA_SIZE:\n",
    "        X = X.sample(MAX_DATA_SIZE, random_state=RND_SEED)\n",
    "        y = y.sample(MAX_DATA_SIZE, random_state=RND_SEED)\n",
    "\n",
    "        all_data_dict[data_name] = (X, y)\n",
    "\n",
    "size_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on dataset wdbc ...\n",
      "  Doing knn magic ...\n",
      "  Doing rf magic ...\n",
      "Working on dataset income ...\n",
      "  Doing knn magic ...\n",
      "  Doing rf magic ...\n",
      "Working on dataset iris ...\n",
      "  Doing knn magic ...\n",
      "  Doing rf magic ...\n",
      "ParamSearcher: rf: some max_features > n_features. Restricting range to max_features.\n",
      "Working on dataset covtype ...\n",
      "  Doing knn magic ...\n",
      "  Doing rf magic ...\n",
      "Working on dataset letter ...\n",
      "  Doing knn magic ...\n",
      "  Doing rf magic ...\n"
     ]
    }
   ],
   "source": [
    "### Go!\n",
    "# clf_param_dict = {'knn':    {'n_neighbors': np.arange(1, 51)},\n",
    "#                   'logreg': {'C': [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]},\n",
    "#                   'rf':     {'n_estimators': [1024],\n",
    "#                              'max_features': [1, 2, 4, 6, 8, 12, 16, 20]}}\n",
    "# clf_param_dict = {'knn':    {'n_neighbors':  np.arange(1, 51)},\n",
    "#                   'rf':     {'n_estimators': np.array([1024]),\n",
    "#                              'max_features': np.array([1, 2, 4, 6, 8, 12, 16, 20])},\n",
    "#                   'svm':    {'kernel':       ['rbf', 'linear']}}\n",
    "clf_param_dict = {'knn':    {'n_neighbors':  np.arange(1, 51)},\n",
    "                  'rf':     {'n_estimators': np.array([256]),\n",
    "                             'max_features': np.array([1, 2, 4, 6])}}\n",
    "\n",
    "\n",
    "everything = MagicSearcher(clf_param_dict, all_data_dict, cv=2, n_jobs=None, verbose=False, method='grid_search')\n",
    "everything.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To do\n",
    "# - Split data to training and testing sets\n",
    "# - Implement MS fit function that uses best parameters, runs on data dict\n",
    "# - Implement MS predict function that, runs on data dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
