{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import neighbors\n",
    "from sklearn import neural_network\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "RND_SEED        = 0\n",
    "CLF_DICT        = {'svm': svm.SVC(),\n",
    "                   'dt':  tree.DecisionTreeClassifier(),\n",
    "                   'rf':  ensemble.RandomForestClassifier(),\n",
    "                   'knn': neighbors.KNeighborsClassifier(),\n",
    "                   'ann': neural_network.MLPClassifier()}\n",
    "\n",
    "CLF_PARAM_DICT = {'svm': {'kernel': ['linear'],\n",
    "                          'C': [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "                  'dt':  {'criterion': ['entropy'],\n",
    "                          'max_depth': [1, 2, 3, 4, 5]},\n",
    "                  'rf':  {'max_features': [1, 2, 4]},\n",
    "                  'knn': {'n_neighbors': [1, 2, 3, 4, 5]},\n",
    "                  'ann': {'hidden_layer_sizes': [(1,), (2,), (4,), (8,), (32,), (128,)]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_test(data, ratio):\n",
    "    \"\"\"\n",
    "    Splits dataset into training and test set of specified size.\n",
    "    \n",
    "        data:  The data to be split.\n",
    "        \n",
    "        ratio: Ratio of first (training) subset.\n",
    "        \n",
    "    Returns two datasets: training and test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_num  = int(np.round(ratio*data.shape[0]))\n",
    "    data_train = data[:train_num]\n",
    "    data_test  = data[train_num:]\n",
    "    return(data_train, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(df):\n",
    "    \"\"\"\n",
    "    Shuffles dataset using seed specified in RND_SEED (see config part above).\n",
    "    \n",
    "        df:  Dataset to be shuffled.\n",
    "        \n",
    "    Returns shuffled dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    return(df.sample(frac=1, random_state=np.random.RandomState(seed=RND_SEED)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, prepare, and shuffle breast cancer data\n",
    "wdbc_X_and_y = pd.read_csv('data/wdbc.data', header = None).iloc[:, 1:] # drop ID, then first col = y\n",
    "wdbc_X_and_y = shuffle(wdbc_X_and_y)\n",
    "wdbc_y = wdbc_X_and_y.iloc[:, 0]\n",
    "wdbc_X = wdbc_X_and_y.iloc[:, 1:]\n",
    "\n",
    "# Transform y from (B, M) to (-1, 1)\n",
    "wdbc_y = wdbc_y.map({'B': -1, 'M': 1})\n",
    "\n",
    "# Split to 80% training and 20% test set\n",
    "wdbc_X_train, wdbc_X_test = split_train_test(wdbc_X, 0.8)\n",
    "wdbc_y_train, wdbc_y_test = split_train_test(wdbc_y, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, prepare, and shuffle adult income data\n",
    "income_X_and_y = pd.read_csv('data/adult.data', header=None)\n",
    "income_X_and_y.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "                         'marital-status', 'occupation', 'relationship',\n",
    "                         'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "                         'native-country', 'income']\n",
    "income_X_and_y = shuffle(income_X_and_y)\n",
    "\n",
    "# one-hot encode categorical variables\n",
    "income_categorical_vars = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                           'relationship', 'race', 'sex', 'native-country']\n",
    "income_X_and_y_onehot = pd.DataFrame()\n",
    "for var in income_categorical_vars:\n",
    "    dummy_coded_var_df = pd.get_dummies(income_X_and_y[var], prefix=var)\n",
    "    income_X_and_y_onehot = pd.concat([income_X_and_y_onehot, dummy_coded_var_df], axis=1)\n",
    "\n",
    "# add remaining columns to one-hot encoded df\n",
    "income_X_and_y = pd.concat([income_X_and_y_onehot,\n",
    "                            income_X_and_y.loc[:, income_X_and_y.columns[\n",
    "                                np.logical_not(np.in1d(income_X_and_y.columns, income_categorical_vars))]]],\n",
    "                           axis=1)\n",
    "\n",
    "income_y = income_X_and_y.loc[:, 'income']\n",
    "income_X = income_X_and_y.drop('income', axis=1)\n",
    "\n",
    "# Transform y from (<=50K, >50K) to (-1, 1)\n",
    "income_y = income_y.map({' <=50K': -1, ' >50K': 1})\n",
    "\n",
    "# Split to 80% training and 20% test set\n",
    "income_X_train, income_X_test = split_train_test(income_X, 0.8)\n",
    "income_y_train, income_y_test = split_train_test(income_y, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_clf(clf_code):\n",
    "    if clf_code in CLF_DICT:\n",
    "        return(CLF_DICT[clf_code])\n",
    "    else:\n",
    "        raise ValueError('You are probably trying to use a classifier that you haven\\'t implemented yet!')\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_clf_paramgrid(clf_code):\n",
    "    if clf_code in CLF_PARAM_DICT:\n",
    "        return(CLF_PARAM_DICT[clf_code])\n",
    "    else:\n",
    "        raise ValueError('I don\\'t know the param grid for your classifier yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_hyperparameters(data_X_train, data_y_train, clf_code_list, cv_fold):\n",
    "    \"\"\"\n",
    "    Returns tuned hyperparameters for a list of classifiers.\n",
    "    Classifier objects and hyperparameters to be tuned are specified\n",
    "      in CLF_DICT and CLF_PARAM_DICT (see config part above).\n",
    "    \n",
    "        data_X_train:  training data X; shape: (N, k)\n",
    "        \n",
    "        data_y_train:  training data y; shape: (N, )\n",
    "        \n",
    "        clf_code_list: list of classifier codes (e.g., ['svm', 'dt', 'rf'])\n",
    "        \n",
    "        cv_fold:       fold parameter for cross-validation\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(clf_code_list, str):\n",
    "        if clf_code_list == 'all':\n",
    "            clf_code_list = list(CLF_DICT.keys())\n",
    "        else:\n",
    "            clf_code_list = [clf_code_list]\n",
    "    \n",
    "    learned_hyperparam_dict = dict()\n",
    "    \n",
    "    for clf_code in clf_code_list:\n",
    "        gs_clf = GridSearchCV(init_clf(clf_code), get_clf_paramgrid(clf_code), cv=cv_fold, n_jobs=4)\n",
    "        gs_clf.fit(data_X_train, data_y_train)\n",
    "        learned_hyperparam_dict[clf_code] = gs_clf.best_params_\n",
    "    \n",
    "    return(learned_hyperparam_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracies(data_X_train, data_y_train, data_X_test, data_y_test, clf_code_list, learned_hyperparams_dict):\n",
    "    \n",
    "    accuracy_dict = dict()\n",
    "    \n",
    "    for clf_code in clf_code_list:\n",
    "        clf = init_clf(clf_code)\n",
    "        learned_hyperparams = learned_hyperparams_dict[clf_code]\n",
    "        \n",
    "        for param_name, param_val in learned_hyperparams.items():\n",
    "            setattr(clf, param_name, param_val)\n",
    "\n",
    "        clf.fit(data_X_train, data_y_train)\n",
    "\n",
    "        train_preds = clf.predict(data_X_train)\n",
    "        test_preds  = clf.predict(data_X_test)\n",
    "\n",
    "        train_acc   = sum(train_preds == data_y_train)/len(data_y_train)\n",
    "        test_acc    = sum(test_preds == data_y_test)/len(data_y_test)\n",
    "        \n",
    "        accuracy_dict[clf_code] = train_acc, test_acc\n",
    "    \n",
    "    return(accuracy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxi/.anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/maxi/.anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/maxi/.anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/maxi/.anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/maxi/.anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ann': {'hidden_layer_sizes': (2,)},\n",
       " 'dt': {'criterion': 'entropy', 'max_depth': 4},\n",
       " 'knn': {'n_neighbors': 5},\n",
       " 'rf': {'max_features': 4},\n",
       " 'svm': {'C': 0.01, 'kernel': 'linear'}}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_hyperparams_wdbc = learn_hyperparameters(wdbc_X_train, wdbc_y_train, ['svm', 'dt', 'rf', 'knn', 'ann'], 5)\n",
    "learned_hyperparams_wdbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_hyperparams_income = learn_hyperparameters(income_X_train, income_y_train, 'all', 5)\n",
    "learned_hyperparams_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ann': (0.93626373626373627, 0.92105263157894735),\n",
       " 'dt': (0.97802197802197799, 0.92105263157894735),\n",
       " 'knn': (0.94285714285714284, 0.93859649122807021),\n",
       " 'rf': (1.0, 0.94736842105263153),\n",
       " 'svm': (0.95824175824175828, 0.96491228070175439)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracies(wdbc_X_train, wdbc_y_train, wdbc_X_test, wdbc_y_test, ['svm', 'dt', 'rf', 'knn', 'ann'], learned_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ann\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxi/.anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/maxi/.anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/maxi/.anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "learned_hyperparams = learn_hyperparameters(wdbc_X_train, wdbc_y_train, 'ann', 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
