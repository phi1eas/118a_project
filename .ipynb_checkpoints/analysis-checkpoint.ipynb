{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes/To do:\n",
    "- in SVM hyperparameter study, generic init_clf is not used but instead svm.SVC(), since for some reason init_clf does not work on svm.SVC. NOTE THAT THIS IS HARD-CODED IN HYPERPARAM_EXPLORER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import neighbors\n",
    "from sklearn import neural_network\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from hyperparams import hyperparam_explorer # my package\n",
    "from hyperparams import hyperparam_cv_gs\n",
    "from hyperparams import init_clf\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "RND_SEED        = 0\n",
    "CLF_DICT        = {'svm': svm.SVC(),\n",
    "                   'dt':  tree.DecisionTreeClassifier(),\n",
    "                   'rf':  ensemble.RandomForestClassifier(),\n",
    "                   'knn': neighbors.KNeighborsClassifier(),\n",
    "                   'ann': neural_network.MLPClassifier()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def shuffle(df):\n",
    "    \"\"\"\n",
    "    Shuffles dataset using seed specified in RND_SEED (see config part above).\n",
    "    \n",
    "        df:  Dataset to be shuffled.\n",
    "        \n",
    "    Returns shuffled dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    return(df.sample(frac=1, random_state=np.random.RandomState(seed=RND_SEED)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## wdbc: Wisconsin Breast Cancer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wdbc total sample size: 569\n"
     ]
    }
   ],
   "source": [
    "# Load, prepare, and shuffle breast cancer data\n",
    "wdbc_X_and_y = pd.read_csv('data/wdbc.data', header = None).iloc[:, 1:] # drop ID, then first col = y\n",
    "wdbc_X_and_y = shuffle(wdbc_X_and_y)\n",
    "wdbc_y = wdbc_X_and_y.iloc[:, 0]\n",
    "wdbc_X = wdbc_X_and_y.iloc[:, 1:]\n",
    "\n",
    "print(\"wdbc total sample size: {}\".format(wdbc_X.shape[0]))\n",
    "\n",
    "# Transform y from (B, M) to (-1, 1)\n",
    "wdbc_y = wdbc_y.map({'B': -1, 'M': 1})\n",
    "\n",
    "# Split to 80% training and 20% test set\n",
    "wdbc_X_train, wdbc_X_test, wdbc_y_train, wdbc_y_test = \\\n",
    "    model_selection.train_test_split(wdbc_X, wdbc_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## income: US Census Income data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Income total sample size: 32561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxi/.anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load, prepare, and shuffle adult income data\n",
    "income_X_and_y = pd.read_csv('data/adult.data', header=None)\n",
    "income_X_and_y.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "                         'marital-status', 'occupation', 'relationship',\n",
    "                         'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "                         'native-country', 'income']\n",
    "income_X_and_y = shuffle(income_X_and_y)\n",
    "print(\"Income total sample size: {}\".format(income_X_and_y.shape[0]))\n",
    "\n",
    "# one-hot encode categorical variables\n",
    "income_categorical_vars = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                           'relationship', 'race', 'sex', 'native-country']\n",
    "income_X_and_y_onehot = pd.DataFrame()\n",
    "for var in income_categorical_vars:\n",
    "    dummy_coded_var_df = pd.get_dummies(income_X_and_y[var], prefix=var)\n",
    "    income_X_and_y_onehot = pd.concat([income_X_and_y_onehot, dummy_coded_var_df], axis=1)\n",
    "\n",
    "# add remaining columns to one-hot encoded df\n",
    "income_X_and_y = pd.concat([income_X_and_y_onehot,\n",
    "                            income_X_and_y.loc[:, income_X_and_y.columns[\n",
    "                                np.logical_not(np.in1d(income_X_and_y.columns, income_categorical_vars))]]],\n",
    "                           axis=1)\n",
    "\n",
    "income_y = income_X_and_y.loc[:, 'income']\n",
    "income_X = income_X_and_y.drop('income', axis=1)\n",
    "\n",
    "# Transform y from (<=50K, >50K) to (-1, 1)\n",
    "income_y = income_y.map({' <=50K': -1, ' >50K': 1})\n",
    "\n",
    "# Split to 80% training and 20% test set\n",
    "income_X_train, income_X_test, income_y_train, income_y_test = \\\n",
    "    model_selection.train_test_split(income_X, income_y, test_size = 0.2)\n",
    "\n",
    "# Hard-limit size of training set to 5000 for computational reasons\n",
    "income5000_X_train, income5000_X_test, income5000_y_train, income5000_y_test = \\\n",
    "    model_selection.train_test_split(income_X, income_y, train_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris total sample size: 150\n"
     ]
    }
   ],
   "source": [
    "iris_X = datasets.load_iris()['data']\n",
    "iris_y = datasets.load_iris()['target']\n",
    "iris_X_train, iris_X_test, iris_y_train, iris_y_test = \\\n",
    "    model_selection.train_test_split(iris_X, iris_y, test_size=0.2)\n",
    "print(\"Iris total sample size: {}\".format(iris_X.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Basic study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# data_dict       = {'wdbc':   [wdbc_X_train, wdbc_y_train]}\n",
    "\n",
    "# hyperparam_dict = {'knn': {'n_neighbors': np.arange(1, 51)}}\n",
    "\n",
    "# hp_cv_gs = hyperparam_cv_gs(data_dict, hyperparam_dict, CLF_DICT)\n",
    "# hp_cv_gs.search()\n",
    "# hp_cv_gs.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## knn: k; income5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# data_dict       = {'income5000': [income5000_X_train, income5000_y_train]}\n",
    "# hyperparam_dict = {'knn' : {'n_neighbors': np.arange(1, 51)}}\n",
    "\n",
    "# hp_cv_gs = hyperparam_cv_gs(data_dict, hyperparam_dict, CLF_DICT)\n",
    "# hp_cv_gs.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# hp_cv_gs.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Run hyperparam study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## knn: k (1:50); wdbc/income_full/iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# data_dict       = {'wdbc':   [wdbc_X_train, wdbc_y_train],\n",
    "#                    'income': [income_X_train, income_y_train],\n",
    "#                    'iris':   [iris_X_train, iris_y_train]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# hyperparam_dict = {'knn': {'n_neighbors': np.arange(1, 51)}}\n",
    "# expl = hyperparam_explorer(data_dict, hyperparam_dict, CLF_DICT, cv=5)\n",
    "# expl.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# result_dict = expl.get_results()\n",
    "# result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# expl.save('wdbc_income_iris_knn_k_1_50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## svm: C (1e-7:1e3) with kernel='linear'; wdbc/income_5000/iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# data_dict       = {'wdbc':   [wdbc_X_train, wdbc_y_train],\n",
    "#                    'income': [income5000_X_train, income5000_y_train],\n",
    "#                    'iris':   [iris_X_train, iris_y_train]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# hyperparam_dict = {'svm': {'C': [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3]}}\n",
    "# expl = hyperparam_explorer(data_dict, hyperparam_dict, CLF_DICT, cv=5)\n",
    "# expl.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# expl.save('wdbc_income5000_iris_svm_linear_C_1e-7_1e3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## knn: k (1:50); wdbc/income_full/iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "knn_expl = hyperparam_explorer.load('./wdbc_income_iris_knn_k_1_50')\n",
    "knn_expl_results = knn_expl.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'expl_plotter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-80c6d3cc7426>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexpl_plotter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknn_expl_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'expl_plotter' is not defined"
     ]
    }
   ],
   "source": [
    "expl_plotter(knn_expl_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## svm: C (1e-7:1e3) with kernel='linear'; wdbc/income_5000/iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "svm_lin_expl = hyperparam_explorer.load('./wdbc_income5000_iris_svm_linear_C_1e-7_1e3')\n",
    "svm_lin_results = svm_lin_expl.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "expl_plotter(svm_lin_results, x_log10=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Package dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def expl_plotter(explorer_results, x_log10=False):\n",
    "    for data_name, data_results in explorer_results.items():\n",
    "        for clf_name, clf_results in data_results.items():\n",
    "            fig, ax = plt.subplots()\n",
    "            \n",
    "            x = np.log10(clf_results.iloc[:, 0]) if(x_log10) else clf_results.iloc[:, 0]\n",
    "            y = clf_results.iloc[:, 1]\n",
    "            \n",
    "            ax.plot(x, y)\n",
    "            \n",
    "            ax.set_title(\"Data: {}\".format(data_name))\n",
    "            \n",
    "            hyperparam_name = clf_results.columns[0]\n",
    "            x_ax_label_format_str = \"log10({})\" if x_log10 == True else \"{}\"\n",
    "            x_ax_label = x_ax_label_format_str.format(hyperparam_name)\n",
    "            ax.set_xlabel(x_ax_label)\n",
    "            \n",
    "            ax.set_ylabel('Cross-validated training accuracy')\n",
    "            \n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "svm_lin_results['income']['svm'].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof that we need to take care of hp interdependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hp_grid = {'kernel': ['linear'], 'degree': [2, 3]}\n",
    "gs = GridSearchCV(svm.SVC(), hp_grid, cv = 5)\n",
    "gs.fit(wdbc_X_train, wdbc_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hp_grid = {'kernel': ['linear']}\n",
    "gs = GridSearchCV(svm.SVC(), hp_grid, cv = 5)\n",
    "gs.fit(wdbc_X_train, wdbc_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clfs_hp_dict = {'svm': {'kernel': ['linear', 'poly', 'rbf'],\n",
    "#                         'degree': [2, 3],\n",
    "#                         'C':      [1e-7, 1e-6, 1e-5],\n",
    "#                         'gamma':  [0.001, 0.005, 0.01]}}\n",
    "# invalid_hp_combinations_dict      = {'svm': [{'key_name': 'kernel',\n",
    "#                                               'key_value': 'linear',\n",
    "#                                               'target_names': ['degree', 'gamma']},\n",
    "#                                              {'key_name': 'kernel',\n",
    "#                                               'key_value': 'rbf',\n",
    "#                                               'target_names': ['degree']}]}\n",
    "clfs_hp_dict = {'svm': {'kernel': ['linear', 'poly', 'rbf'],\n",
    "                        'degree': [2, 3],\n",
    "                        'C':      [10, 100, 1000]}}\n",
    "invalid_hp_combinations_dict      = {'svm': [{'key_name': 'kernel',\n",
    "                                              'key_value': 'linear',\n",
    "                                              'target_names': ['degree', 'C']},\n",
    "                                             {'key_name': 'kernel',\n",
    "                                              'key_value': 'rbf',\n",
    "                                              'target_names': ['degree']}]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_invalid_hp_combinations(clfs_hp_dict, invalid_hp_combinations_dict):\n",
    "    \"\"\"\n",
    "    Returns all combinations of hyperparameters as df for dict of classifiers with invalid/redundant combinations removed.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clfs_hp_dict: dict with clf codes as keys and hyperparam grid dicts as entries\n",
    "    \n",
    "    invalid_hp_combinations_dict: dict with clf codes as keys and lists of rule dicts as entries\n",
    "        Example of list of rule dicts: [{'key_name':     'kernel',\n",
    "                                         'key_value':    'linear',\n",
    "                                         'target_names': ['degree', 'C']},\n",
    "                                        {'key_name':     'kernel',\n",
    "                                         'key_value':    'rbf',\n",
    "                                         'target_names': ['degree']}]\n",
    "        This example will retain only one combination of linear kernels and degrees/C values,\n",
    "        and only one combination of rbf kernels and degrees.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    clfs_filtered_hp_combinations: dict with clf codes as keys and filtered hyperparam combination dfs as values\n",
    "        \n",
    "    \"\"\"\n",
    "    clfs_filtered_hp_combinations = dict()\n",
    "    for clf_name, clf_hp_dict in clfs_hp_dict.items():\n",
    "        hps = sorted(clf_hp_dict)\n",
    "        hp_combinations = np.array(list(it.product(*(clf_hp_dict[Name] for Name in hps))))\n",
    "        hp_combinations = pd.DataFrame(hp_combinations, index=None)\n",
    "        hp_combinations.columns = hps\n",
    "        print(hp_combinations)\n",
    "        # Add dummy column to combinations. This is needed later for filtering if there are now hyperparams that don't occur in the hp combinations to be filtered\n",
    "        hp_combinations['dummy'] = 1\n",
    "        \n",
    "        \n",
    "        all_redundant_rows_idx = list()\n",
    "        if clf_name in invalid_hp_combinations_dict.keys():\n",
    "#           for hp_key_name, hp_key_values in invalid_hp_combinations_dict[clf_name].items():\n",
    "            for rule in invalid_hp_combinations_dict[clf_name]:\n",
    "                hp_key_name = rule['key_name']\n",
    "                hp_key_value = rule['key_value']\n",
    "                hp_target_names = rule['target_names']\n",
    "                \n",
    "#                 for hp_key_value, hp_targets in hp_key_values.items():\n",
    "                # Get only combinations where hp_key_name == hp_key_value\n",
    "                only_hp_key_value_combinations = hp_combinations.loc[hp_combinations[hp_key_name] == hp_key_value]\n",
    "                print(only_hp_key_value_combinations)\n",
    "                for hp_target_name in hp_target_names:\n",
    "                    # Show only OTHER hp columns\n",
    "                    other_hps = np.setdiff1d(hp_combinations.columns, [hp_key_name, hp_target_name])\n",
    "                    only_hp_key_value_combinations_other_hps = only_hp_key_value_combinations.loc[:, other_hps]\n",
    "                    print(only_hp_key_value_combinations_other_hps)\n",
    "                    # Get duplicate rows - these are the combinations where values of the invalid parameter vary, we don't want to compute these\n",
    "                    redundant_rows = only_hp_key_value_combinations_other_hps.duplicated()\n",
    "                    redundant_rows_idx = redundant_rows.index[redundant_rows].tolist()\n",
    "                    print(redundant_rows_idx)\n",
    "                    # Save the row indices\n",
    "                    all_redundant_rows_idx.extend(redundant_rows_idx)\n",
    "\n",
    "        all_redundant_rows_idx = list(set(all_redundant_rows_idx))\n",
    "\n",
    "        # Drop'em\n",
    "        filtered_hp_combinations = hp_combinations.drop(hp_combinations.index[all_redundant_rows_idx])\n",
    "        \n",
    "        # Drop dummy column\n",
    "        filtered_hp_combinations = filtered_hp_combinations.drop('dummy', axis=1)\n",
    "        \n",
    "        # Automatically infer and convert to dtypes (which get lost somewhere above)\n",
    "        filtered_hp_combinations = filtered_hp_combinations.convert_objects(convert_numeric=True)\n",
    "        \n",
    "        clfs_filtered_hp_combinations[clf_name] = filtered_hp_combinations\n",
    "    \n",
    "    return(clfs_filtered_hp_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 39 µs, total: 39 µs\n",
      "Wall time: 42 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class MyGridSearchCV:\n",
    "    \n",
    "    clfs_hp_dict = None\n",
    "    clfs_invalid_hp_combinations_dict = None\n",
    "    cv = None\n",
    "    verbose = None\n",
    "    \n",
    "    clfs_hp_filtered_combinations_dict = None\n",
    "    \n",
    "    clfs_hp_combinations_results_dict = None\n",
    "    \n",
    "    def __init__(self, clfs_hp_dict, clfs_invalid_hp_combinations_dict, cv=None, n_jobs=1, verbose=False):\n",
    "        self.clfs_hp_dict = clfs_hp_dict\n",
    "        self.clfs_invalid_hp_combinations_dict = clfs_invalid_hp_combinations_dict\n",
    "        self.cv = cv\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.clfs_hp_filtered_combinations_dict = \\\n",
    "            remove_invalid_hp_combinations(clfs_hp_dict, clfs_invalid_hp_combinations_dict)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        clfs_hp_combinations_results_dict = dict()\n",
    "        for clf_name, clf_hp_combinations_df in self.clfs_hp_filtered_combinations_dict.items():\n",
    "\n",
    "            clf_hp_combinations_scores_mean = []\n",
    "            clf_hp_combinations_scores_sd = []\n",
    "            for row_idx, row in clf_hp_combinations_df.iterrows():\n",
    "                clf_hp_combination = row.to_dict()\n",
    "\n",
    "                clf = init_clf(clf_name, CLF_DICT)\n",
    "\n",
    "                # Set clf hyperparams\n",
    "                for hp_name, hp_val in clf_hp_combination.items():\n",
    "                    setattr(clf, hp_name, hp_val)\n",
    "\n",
    "                scores = cross_val_score(clf, X, y, cv=cv, n_jobs=4, verbose=self.verbose)\n",
    "                scores_mean = np.mean(scores)\n",
    "                scores_sd = np.std(scores)\n",
    "                clf_hp_combinations_scores_mean.append(scores_mean)\n",
    "                clf_hp_combinations_scores_sd.append(scores_sd)\n",
    "\n",
    "            clf_hp_combinations_df['scores_mean'] = clf_hp_combinations_scores_mean\n",
    "            clf_hp_combinations_df['scores_sd'] = clf_hp_combinations_scores_sd\n",
    "\n",
    "            clfs_hp_combinations_results_dict[clf_name] = clf_hp_combinations_df\n",
    "        self.clfs_hp_combinations_results_dict = clfs_hp_combinations_results_dict\n",
    "        self.__find_best_params()\n",
    "    \n",
    "    def __find_best_params(self):\n",
    "        best_params = dict()\n",
    "        for clf_name, clf_hp_combinations_results_df in self.clfs_hp_combinations_results_dict.items():\n",
    "            clf_best_params = clf_hp_combinations_results_df.iloc[clf_hp_combinations_results_df['scores_mean'].idxmax(), :-2].to_dict()\n",
    "            best_params[clf_name] = clf_best_params\n",
    "        self.best_params_ = best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfs_hp_dict = {'svm': {'kernel': ['linear', 'poly', 'rbf'],\n",
    "                        'degree': [2, 3],\n",
    "                        'C':      [1e-7, 1e-6, 1e-5],\n",
    "                        'gamma':  [0.001, 0.005, 0.01]}}\n",
    "clfs_invalid_hp_combinations_dict = {'svm': {'kernel': {'linear': ['degree', 'gamma']},\n",
    "                                             'kernel': {'rbf':    ['degree']}}}\n",
    "\n",
    "                                  = {'svm': [{'key_name': 'kernel',\n",
    "                                              'key_value': 'linear',\n",
    "                                              'target_names': ['degree', 'gamma']},\n",
    "                                             {'key_name': 'kernel',\n",
    "                                              'key_value': 'rbf',\n",
    "                                              'target_names': ['degree']}]}\n",
    "\n",
    "X = iris_X_train\n",
    "y = iris_y_train\n",
    "\n",
    "cv = 2\n",
    "n_jobs = 4\n",
    "verbose = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': {'rbf': ['degree']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxi/.anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:56: FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n"
     ]
    }
   ],
   "source": [
    "test = remove_invalid_hp_combinations(clfs_hp_dict, clfs_invalid_hp_combinations_dict)['svm']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxi/.anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:55: FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.39 s, sys: 1.17 s, total: 3.56 s\n",
      "Wall time: 8.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gs = MyGridSearchCV(clfs_hp_dict, clfs_invalid_hp_combinations_dict, cv=cv, n_jobs=n_jobs)\n",
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svm': {'C': 1.0000000000000001e-05,\n",
       "  'degree': 3,\n",
       "  'gamma': 0.001,\n",
       "  'kernel': 'rbf'}}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "[CV] degree=2, kernel=linear .........................................\n",
      "[CV] degree=2, kernel=linear .........................................\n",
      "[CV] degree=2, kernel=poly ...........................................\n",
      "[CV] degree=2, kernel=poly ...........................................\n",
      "[CV]  degree=2, kernel=linear, score=0.9517543859649122, total=   0.4s\n",
      "[CV] degree=3, kernel=linear .........................................\n",
      "[CV]  degree=3, kernel=linear, score=0.9517543859649122, total=   0.4s\n",
      "[CV] degree=3, kernel=linear .........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   8 | elapsed:    0.8s remaining:    2.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  degree=2, kernel=linear, score=0.9559471365638766, total=   1.5s\n",
      "[CV] degree=3, kernel=poly ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   3 out of   8 | elapsed:    1.6s remaining:    2.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  degree=3, kernel=linear, score=0.9559471365638766, total=   1.5s\n",
      "[CV] degree=3, kernel=poly ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   4 out of   8 | elapsed:    2.3s remaining:    2.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ... degree=3, kernel=poly, score=0.920704845814978, total=  20.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 out of   8 | elapsed:   23.2s remaining:   13.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. degree=2, kernel=poly, score=0.9385964912280702, total=  29.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   6 out of   8 | elapsed:   29.8s remaining:    9.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .. degree=3, kernel=poly, score=0.9517543859649122, total=  30.8s\n",
      "[CV] .. degree=2, kernel=poly, score=0.9515418502202643, total=  41.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   8 out of   8 | elapsed:   41.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   8 out of   8 | elapsed:   41.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.42 s, sys: 63.8 ms, total: 1.48 s\n",
      "Wall time: 43.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gs_ = GridSearchCV(svm.SVC(), clfs_hp_dict['svm'], cv=cv, n_jobs=n_jobs, verbose=verbose)\n",
    "gs_.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'degree': 2, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?svm.SVC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
