{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import neighbors\n",
    "from sklearn import neural_network\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "RND_SEED        = 0\n",
    "CLF_DICT        = {'svm': svm.SVC(),\n",
    "                   'dt':  tree.DecisionTreeClassifier(),\n",
    "                   'rf':  ensemble.RandomForestClassifier(),\n",
    "                   'knn': neighbors.KNeighborsClassifier(),\n",
    "                   'ann': neural_network.MLPClassifier()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_test(data, ratio):\n",
    "    \"\"\"\n",
    "    Splits dataset into training and test set of specified size.\n",
    "    \n",
    "        data:  The data to be split.\n",
    "        \n",
    "        ratio: Ratio of first (training) subset.\n",
    "        \n",
    "    Returns two datasets: training and test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_num  = int(np.round(ratio*data.shape[0]))\n",
    "    data_train = data[:train_num]\n",
    "    data_test  = data[train_num:]\n",
    "    return(data_train, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(df):\n",
    "    \"\"\"\n",
    "    Shuffles dataset using seed specified in RND_SEED (see config part above).\n",
    "    \n",
    "        df:  Dataset to be shuffled.\n",
    "        \n",
    "    Returns shuffled dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    return(df.sample(frac=1, random_state=np.random.RandomState(seed=RND_SEED)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, prepare, and shuffle breast cancer data\n",
    "wdbc_X_and_y = pd.read_csv('data/wdbc.data', header = None).iloc[:, 1:] # drop ID, then first col = y\n",
    "wdbc_X_and_y = shuffle(wdbc_X_and_y)\n",
    "wdbc_y = wdbc_X_and_y.iloc[:, 0]\n",
    "wdbc_X = wdbc_X_and_y.iloc[:, 1:]\n",
    "\n",
    "# Transform y from (B, M) to (-1, 1)\n",
    "wdbc_y = wdbc_y.map({'B': -1, 'M': 1})\n",
    "\n",
    "# Split to 80% training and 20% test set\n",
    "wdbc_X_train, wdbc_X_test = split_train_test(wdbc_X, 0.8)\n",
    "wdbc_y_train, wdbc_y_test = split_train_test(wdbc_y, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, prepare, and shuffle adult income data\n",
    "income_X_and_y = pd.read_csv('data/adult.data', header=None)\n",
    "income_X_and_y.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "                         'marital-status', 'occupation', 'relationship',\n",
    "                         'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "                         'native-country', 'income']\n",
    "income_X_and_y = shuffle(income_X_and_y)\n",
    "\n",
    "# one-hot encode categorical variables\n",
    "income_categorical_vars = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                           'relationship', 'race', 'sex', 'native-country']\n",
    "income_X_and_y_onehot = pd.DataFrame()\n",
    "for var in income_categorical_vars:\n",
    "    dummy_coded_var_df = pd.get_dummies(income_X_and_y[var], prefix=var)\n",
    "    income_X_and_y_onehot = pd.concat([income_X_and_y_onehot, dummy_coded_var_df], axis=1)\n",
    "\n",
    "# add remaining columns to one-hot encoded df\n",
    "income_X_and_y = pd.concat([income_X_and_y_onehot,\n",
    "                            income_X_and_y.loc[:, income_X_and_y.columns[\n",
    "                                np.logical_not(np.in1d(income_X_and_y.columns, income_categorical_vars))]]],\n",
    "                           axis=1)\n",
    "\n",
    "income_y = income_X_and_y.loc[:, 'income']\n",
    "income_X = income_X_and_y.drop('income', axis=1)\n",
    "\n",
    "# Transform y from (<=50K, >50K) to (-1, 1)\n",
    "income_y = income_y.map({' <=50K': -1, ' >50K': 1})\n",
    "\n",
    "# Split to 80% training and 20% test set\n",
    "income_X_train, income_X_test = split_train_test(income_X, 0.8)\n",
    "income_y_train, income_y_test = split_train_test(income_y, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_clf(clf_code):\n",
    "    if clf_code in CLF_DICT:\n",
    "        return(CLF_DICT[clf_code])\n",
    "    else:\n",
    "        raise ValueError('You are probably trying to use a classifier that you haven\\'t implemented yet!')\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class hyperparam_learner:\n",
    "    data_X_train  = None\n",
    "    data_y_train  = None\n",
    "    data_desc     = None\n",
    "    clf_code_list = None\n",
    "    cv_fold       = None\n",
    "    \n",
    "    hyperparam_dict = None\n",
    "    \n",
    "    is_fitted     = False\n",
    "    has_learned   = False\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        with open(filename + '.pkl', 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    def save(self, filename):\n",
    "        if not self.is_fitted:\n",
    "            raise Exception('I am not fitted yet!')\n",
    "        if not self.has_learned:\n",
    "            raise Exception('I have not learned any hyperparameters yet!')\n",
    "        \n",
    "        with open(filename + '.pkl', 'wb') as f:\n",
    "            pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    def fit(self, data_X_train, data_y_train, data_desc, clf_code_list, cv_fold):\n",
    "        self.data_X_train   = data_X_train\n",
    "        self.data_y_train   = data_y_train\n",
    "        self.data_desc      = data_desc\n",
    "        self.clf_code_list  = clf_code_list\n",
    "        self.cv_fold        = cv_fold\n",
    "        self.is_fitted      = True\n",
    "    \n",
    "    def learn_hyperparams(self, clf_param_grid):\n",
    "        if(not self.is_fitted):\n",
    "            raise Exception('I am not fitted yet!')\n",
    "            \n",
    "        if isinstance(self.clf_code_list, str):\n",
    "            if self.clf_code_list == 'all':\n",
    "                self.clf_code_list = list(CLF_DICT.keys())\n",
    "            else:\n",
    "                self.clf_code_list = [self.clf_code_list]\n",
    "\n",
    "        learned_hyperparam_dict = dict()\n",
    "        considered_hyperparam_dict = dict()\n",
    "\n",
    "        for clf_code in self.clf_code_list:\n",
    "            considered_hyperparam_dict[clf_code] = clf_param_grid[clf_code]\n",
    "            gs_clf = GridSearchCV(init_clf(clf_code), clf_param_grid[clf_code], cv=self.cv_fold, n_jobs=4)\n",
    "            gs_clf.fit(self.data_X_train, self.data_y_train)\n",
    "            learned_hyperparam_dict[clf_code] = gs_clf.best_params_\n",
    "\n",
    "        hyperparam_dict = {'learned_hyperparams'    : learned_hyperparam_dict,\n",
    "                           'considered_hyperparams' : considered_hyperparam_dict,\n",
    "                           'data_desc'              : self.data_desc}\n",
    "        \n",
    "        self.hyperparam_dict = hyperparam_dict\n",
    "        self.has_learned = True\n",
    "        return(self.hyperparam_dict)\n",
    "    \n",
    "    def get_hyperparams(self):\n",
    "        if(self.has_learned):\n",
    "            return(self.hyperparam_dict)\n",
    "        else:\n",
    "            raise Exception('I have not learned any hyperparameters yet!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracies(data_X_train, data_y_train, data_X_test, data_y_test, clf_code_list, learned_hyperparams_dict):\n",
    "    \n",
    "    accuracy_dict = dict()\n",
    "    \n",
    "    for clf_code in clf_code_list:\n",
    "        clf = init_clf(clf_code)\n",
    "        learned_hyperparams = learned_hyperparams_dict[clf_code]\n",
    "        \n",
    "        for param_name, param_val in learned_hyperparams.items():\n",
    "            setattr(clf, param_name, param_val)\n",
    "\n",
    "        clf.fit(data_X_train, data_y_train)\n",
    "\n",
    "        train_preds = clf.predict(data_X_train)\n",
    "        test_preds  = clf.predict(data_X_test)\n",
    "\n",
    "        train_acc   = sum(train_preds == data_y_train)/len(data_y_train)\n",
    "        test_acc    = sum(test_preds == data_y_test)/len(data_y_test)\n",
    "        \n",
    "        accuracy_dict[clf_code] = train_acc, test_acc\n",
    "    \n",
    "    return(accuracy_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define hyperparameter grid to be considered\n",
    "clf_param_grid = {'svm': {'kernel': ['linear'],\n",
    "                          'C': [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]},\n",
    "                  'dt':  {'criterion': ['entropy'],\n",
    "                          'max_depth': [1, 2, 3, 4, 5]},\n",
    "                  'rf':  {'max_features': [1, 2, 4]},\n",
    "                  'knn': {'n_neighbors': [1, 2, 3, 4, 5]},\n",
    "                  'ann': {'hidden_layer_sizes': [(1,), (2,), (4,), (8,), (32,), (128,)]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxi/.anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'considered_hyperparams': {'ann': {'hidden_layer_sizes': [(1,),\n",
       "    (2,),\n",
       "    (4,),\n",
       "    (8,),\n",
       "    (32,),\n",
       "    (128,)]},\n",
       "  'dt': {'criterion': ['entropy'], 'max_depth': [1, 2, 3, 4, 5]},\n",
       "  'knn': {'n_neighbors': [1, 2, 3, 4, 5]},\n",
       "  'rf': {'max_features': [1, 2, 4]},\n",
       "  'svm': {'C': [1e-07, 1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1],\n",
       "   'kernel': ['linear']}},\n",
       " 'data_desc': 'wdbc_80',\n",
       " 'learned_hyperparams': {'ann': {'hidden_layer_sizes': (4,)},\n",
       "  'dt': {'criterion': 'entropy', 'max_depth': 5},\n",
       "  'knn': {'n_neighbors': 5},\n",
       "  'rf': {'max_features': 1},\n",
       "  'svm': {'C': 0.01, 'kernel': 'linear'}}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learn hyperparameters based on breast cancer data\n",
    "wdbc_learner = hyperparam_learner()\n",
    "wdbc_learner.fit(data_X_train = wdbc_X_train,\n",
    "                 data_y_train = wdbc_y_train,\n",
    "                 data_desc = 'wdbc_80', # 80% training\n",
    "                 clf_code_list = 'all',\n",
    "                 cv_fold = 5)\n",
    "wdbc_learner.learn_hyperparams(clf_param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wdbc_learner.save('wdbc_learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = hyperparam_learner.load('wdbc_learner')\n",
    "test.get_hyperparams()['learned_hyperparams']['svm']['C']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
