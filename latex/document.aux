\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\bibstyle{plainnat}
\citation{andrychowicz_learning_2016}
\citation{caruana_empirical_2006}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{dheeru_uci_2017}
\citation{mangasarian_cancer_1990}
\@writefile{toc}{\contentsline {section}{\numberline {2}Method}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Problems}{2}{subsection.2.1}}
\@writefile{toc}{\contentsline {paragraph}{COVTYPE}{2}{subsection.2.1}}
\@writefile{toc}{\contentsline {paragraph}{INCOME}{2}{subsection.2.1}}
\@writefile{toc}{\contentsline {paragraph}{LETTER}{2}{subsection.2.1}}
\@writefile{toc}{\contentsline {paragraph}{IRIS}{2}{subsection.2.1}}
\@writefile{toc}{\contentsline {paragraph}{WDBC}{2}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Classifier Algorithms}{3}{subsection.2.2}}
\newlabel{sec:clf_hyperparams}{{2.2}{3}{Classifier Algorithms}{subsection.2.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Original and limited sizes of data sets\relax }}{3}{table.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:data_sizes}{{1}{3}{Original and limited sizes of data sets\relax }{table.caption.1}{}}
\@writefile{toc}{\contentsline {paragraph}{K Nearest Neighbors (knn)}{3}{subsection.2.2}}
\@writefile{toc}{\contentsline {paragraph}{Logistic Regressoin (logreg)}{3}{subsection.2.2}}
\@writefile{toc}{\contentsline {paragraph}{Decision Tree (dt)}{3}{subsection.2.2}}
\@writefile{toc}{\contentsline {paragraph}{Bagged Decision Trees (bagdt)}{3}{subsection.2.2}}
\@writefile{toc}{\contentsline {paragraph}{Boosted Decision Trees (bstdt)}{3}{subsection.2.2}}
\@writefile{toc}{\contentsline {paragraph}{Random Forest (rf)}{3}{subsection.2.2}}
\citation{caruana_empirical_2006}
\citation{caruana_empirical_2006}
\citation{caruana_empirical_2006}
\citation{caruana_empirical_2006}
\citation{caruana_empirical_2006}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiments}{4}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Prediction Accuracy by Hyperparameter}{4}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Train/Test split and Performance}{4}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Overall Classifier Performance}{4}{subsection.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{4}{section.4}}
\newlabel{fig:hyperparam_knn_zoomed}{{1a}{5}{KNN: \textit {k}\relax }{figure.caption.2}{}}
\newlabel{sub@fig:hyperparam_knn_zoomed}{{a}{5}{KNN: \textit {k}\relax }{figure.caption.2}{}}
\newlabel{fig:hyperparam_knn_all}{{1b}{5}{KNN: \textit {k}\relax }{figure.caption.2}{}}
\newlabel{sub@fig:hyperparam_knn_all}{{b}{5}{KNN: \textit {k}\relax }{figure.caption.2}{}}
\newlabel{fig:hyperparam_dt_max_depth}{{1c}{5}{Decision Tree: \textit {max\_depth}\relax }{figure.caption.2}{}}
\newlabel{sub@fig:hyperparam_dt_max_depth}{{c}{5}{Decision Tree: \textit {max\_depth}\relax }{figure.caption.2}{}}
\newlabel{fig:hyperparam_dt_max_features}{{1d}{5}{Decision Tree: \textit {max\_features}\relax }{figure.caption.2}{}}
\newlabel{sub@fig:hyperparam_dt_max_features}{{d}{5}{Decision Tree: \textit {max\_features}\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Average cross-validated prediction accuracy by hyperparameter value for KNN (\ref  {fig:hyperparam_knn_zoomed} and \ref  {fig:hyperparam_knn_all}) and Decision Tree (\ref  {fig:hyperparam_dt_max_depth} and \ref  {fig:hyperparam_dt_max_features}), averaged over problems, shuffles, and train split. (\ref  {fig:hyperparam_knn_zoomed}) zooms in on a subset of the results shown in (\ref  {fig:hyperparam_dt_max_depth}) for greater detail. (\ref  {fig:hyperparam_dt_max_depth}) depicts results for varying levels of \textit  {max\_depth}, averaged over \textit  {max\_features}, and vice versa in (\ref  {fig:hyperparam_dt_max_depth}).\relax }}{5}{figure.caption.2}}
\newlabel{fig:hyperparam_knn_dt}{{1}{5}{Average cross-validated prediction accuracy by hyperparameter value for KNN (\ref {fig:hyperparam_knn_zoomed} and \ref {fig:hyperparam_knn_all}) and Decision Tree (\ref {fig:hyperparam_dt_max_depth} and \ref {fig:hyperparam_dt_max_features}), averaged over problems, shuffles, and train split. (\ref {fig:hyperparam_knn_zoomed}) zooms in on a subset of the results shown in (\ref {fig:hyperparam_dt_max_depth}) for greater detail. (\ref {fig:hyperparam_dt_max_depth}) depicts results for varying levels of \textit {max\_depth}, averaged over \textit {max\_features}, and vice versa in (\ref {fig:hyperparam_dt_max_depth}).\relax }{figure.caption.2}{}}
\newlabel{fig:hyperparam_logreg}{{2a}{6}{Logistic Regression: \textit {C}\relax }{figure.caption.3}{}}
\newlabel{sub@fig:hyperparam_logreg}{{a}{6}{Logistic Regression: \textit {C}\relax }{figure.caption.3}{}}
\newlabel{fig:hyperparam_rf}{{2b}{6}{Random Forest: \textit {n\_estimators}\relax }{figure.caption.3}{}}
\newlabel{sub@fig:hyperparam_rf}{{b}{6}{Random Forest: \textit {n\_estimators}\relax }{figure.caption.3}{}}
\newlabel{fig:hyperparam_bagdt}{{2c}{6}{Bagged Decision Trees: \textit {n\_estimators}\relax }{figure.caption.3}{}}
\newlabel{sub@fig:hyperparam_bagdt}{{c}{6}{Bagged Decision Trees: \textit {n\_estimators}\relax }{figure.caption.3}{}}
\newlabel{fig:hyperparam_bstdt}{{2d}{6}{Boosted Decision Trees: \textit {n\_estimators}\relax }{figure.caption.3}{}}
\newlabel{sub@fig:hyperparam_bstdt}{{d}{6}{Boosted Decision Trees: \textit {n\_estimators}\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Average cross-validated prediction accuracy by hyperparameter value for Logistic Regression (\ref  {fig:hyperparam_logreg}), Random Forest (\ref  {fig:hyperparam_rf}), Bagged Decision Trees (\ref  {fig:hyperparam_bagdt}), and Boosted Decision Trees (\ref  {fig:hyperparam_bstdt}).\relax }}{6}{figure.caption.3}}
\newlabel{fig:hyperparam_others}{{2}{6}{Average cross-validated prediction accuracy by hyperparameter value for Logistic Regression (\ref {fig:hyperparam_logreg}), Random Forest (\ref {fig:hyperparam_rf}), Bagged Decision Trees (\ref {fig:hyperparam_bagdt}), and Boosted Decision Trees (\ref {fig:hyperparam_bstdt}).\relax }{figure.caption.3}{}}
\newlabel{fig:perf_by_trainsplit_tr}{{3a}{7}{Training accuracy\relax }{figure.caption.4}{}}
\newlabel{sub@fig:perf_by_trainsplit_tr}{{a}{7}{Training accuracy\relax }{figure.caption.4}{}}
\newlabel{fig:perf_by_trainsplit_te}{{3b}{7}{Testing accuracy\relax }{figure.caption.4}{}}
\newlabel{sub@fig:perf_by_trainsplit_te}{{b}{7}{Testing accuracy\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Training (\ref  {fig:perf_by_trainsplit_tr}) and testing (\ref  {fig:perf_by_trainsplit_te}) accuracy by train split and classifier, averaged over problems and shuffles (rf: Random Forest, logreg: Logistic Regression, dt: Decision Tree, bagdt: Bagged Trees, bstdt: Boosted Trees)\relax }}{7}{figure.caption.4}}
\newlabel{fig:perf_by_trainsplit}{{3}{7}{Training (\ref {fig:perf_by_trainsplit_tr}) and testing (\ref {fig:perf_by_trainsplit_te}) accuracy by train split and classifier, averaged over problems and shuffles (rf: Random Forest, logreg: Logistic Regression, dt: Decision Tree, bagdt: Bagged Trees, bstdt: Boosted Trees)\relax }{figure.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Best hyperparameter values after grid search by classifier and problem in each of three random shuffles (0.8 train split):\relax }}{7}{table.caption.5}}
\newlabel{tab:best_hyperparams_by_problem_shuffles}{{2}{7}{Best hyperparameter values after grid search by classifier and problem in each of three random shuffles (0.8 train split):\relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Bonus Points}{7}{section.5}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Classifier testing accuracy by problem, averaged over shuffles (0.8 train split):\relax }}{8}{table.caption.6}}
\newlabel{tab:test_acc_by_clf_and_problem}{{3}{8}{Classifier testing accuracy by problem, averaged over shuffles (0.8 train split):\relax }{table.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Overall average testing accuracy by classifier and comparison with results from \cite  {caruana_empirical_2006}\relax }}{8}{table.caption.7}}
\newlabel{tab:clf_ranking}{{4}{8}{Overall average testing accuracy by classifier and comparison with results from \cite {caruana_empirical_2006}\relax }{table.caption.7}{}}
\newlabel{table:1b}{{4a}{8}{Ranked classifiers by testing accuracy, averaged over problems and shuffles (0.8 train split):\relax }{table.caption.7}{}}
\newlabel{sub@table:1b}{{a}{8}{Ranked classifiers by testing accuracy, averaged over problems and shuffles (0.8 train split):\relax }{table.caption.7}{}}
\bibdata{118a}
\bibcite{andrychowicz_learning_2016}{{1}{2016}{{Andrychowicz et~al.}}{{Andrychowicz, Denil, Gomez, Hoffman, Pfau, Schaul, Shillingford, and de~Freitas}}}
\bibcite{caruana_empirical_2006}{{2}{2006}{{Caruana and Niculescu-Mizil}}{{}}}
\bibcite{dheeru_uci_2017}{{3}{2017}{{Dheeru and Karra~Taniskidou}}{{}}}
\bibcite{mangasarian_cancer_1990}{{4}{1990}{{Mangasarian and Wolberg}}{{}}}
\gdef\minted@oldcachelist{,
  default-pyg-prefix.pygstyle,
  default.pygstyle,
  05338263E1FEF341746F7E6413328AA61C007370217B166059FECBD3DC7D52DE.pygtex}
