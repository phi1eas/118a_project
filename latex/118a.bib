
@techreport{dheeru_uci_2017,
	title = {{UCI} {Machine} {Learning} {Repository}},
	url = {http://archive.ics.uci.edu/ml},
	institution = {University of California, Irvine, School of Information and Computer Sciences},
	author = {Dheeru, Dua and Karra Taniskidou, Efi},
	year = {2017}
}

@article{mangasarian_cancer_1990,
	title = {Cancer diagnosis via linear programming},
	volume = {23},
	number = {5},
	journal = {SIAM News},
	author = {Mangasarian, O. L. and Wolberg, W. H.},
	month = sep,
	year = {1990},
	pages = {1\&18}
}

@article{andrychowicz_learning_2016,
	title = {Learning to learn by gradient descent by gradient descent},
	url = {http://arxiv.org/abs/1606.04474},
	abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
	urldate = {2018-03-20},
	journal = {arXiv:1606.04474 [cs]},
	author = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and Shillingford, Brendan and de Freitas, Nando},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.04474},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1606.04474 PDF:/home/maxi/Zotero/storage/NUH3I4S5/Andrychowicz et al. - 2016 - Learning to learn by gradient descent by gradient .pdf:application/pdf;arXiv.org Snapshot:/home/maxi/Zotero/storage/U6KTMBMC/1606.html:text/html}
}

@inproceedings{caruana_empirical_2006,
	address = {New York, NY, USA},
	series = {{ICML} '06},
	title = {An {Empirical} {Comparison} of {Supervised} {Learning} {Algorithms}},
	isbn = {978-1-59593-383-6},
	url = {http://doi.acm.org/10.1145/1143844.1143865},
	doi = {10.1145/1143844.1143865},
	abstract = {A number of supervised learning methods have been introduced in the last decade. Unfortunately, the last comprehensive empirical evaluation of supervised learning was the Statlog Project in the early 90's. We present a large-scale empirical comparison between ten supervised learning methods: SVMs, neural nets, logistic regression, naive bayes, memory-based learning, random forests, decision trees, bagged trees, boosted trees, and boosted stumps. We also examine the effect that calibrating the models via Platt Scaling and Isotonic Regression has on their performance. An important aspect of our study is the use of a variety of performance criteria to evaluate the learning methods.},
	urldate = {2018-03-20},
	booktitle = {Proceedings of the 23rd {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Caruana, Rich and Niculescu-Mizil, Alexandru},
	year = {2006},
	pages = {161--168}
}